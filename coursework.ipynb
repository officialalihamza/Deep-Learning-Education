#AI Solution for the Garbage detetion
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing import image
from google.colab import drive
from google.colab import files

#Mount Google Drive
drive.mount('/content/drive')

#Dataset Path
dataset_path = "/content/drive/MyDrive/TrashType_Image_Dataset"
if not os.path.isdir(dataset_path):
    raise FileNotFoundError(f"Folder not found: {dataset_path}")

print("Dataset folder:", dataset_path)
print("Class folders:", [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))])

#Parameters
image_height = 224
image_width = 224
batch_size = 32
validation_split = 0.2
random_seed = 42
epochs = 10

#Load Dataset
train_dataset = tf.keras.utils.image_dataset_from_directory(
    dataset_path,
    validation_split=validation_split,
    subset="training",
    seed=random_seed,
    image_size=(image_height, image_width),
    batch_size=batch_size
    )

validation_dataset = tf.keras.utils.image_dataset_from_directory(
    dataset_path,
    validation_split=validation_split,
    subset="validation",
    seed=random_seed,
    image_size=(image_height, image_width),
    batch_size=batch_size
)

class_names = train_dataset.class_names
num_classes = len(class_names)
print("Detected classes:", class_names)


#Performance
AUTOTUNE = tf.data.AUTOTUNE
train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)

#3-Layer CNN Model
cnn_model = models.Sequential([
    layers.Rescaling(1.0 / 255, input_shape=(image_height, image_width, 3)),

    layers.Conv2D(32, (3, 3), activation="relu"),
    layers.MaxPooling2D(),

    layers.Conv2D(64, (3, 3), activation="relu"),
    layers.MaxPooling2D(),

    layers.Conv2D(128, (3, 3), activation="relu"),
    layers.MaxPooling2D(),

    layers.Flatten(),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(num_classes, activation="softmax")
])

cnn_model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

cnn_model.summary()

#Training
early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=3,
    restore_best_weights=True
)

history = cnn_model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=30,
    callbacks=[early_stop]
)

#Save Model
model_path = "/content/garbage_cnn_model.h5"
cnn_model.save(model_path)
print("Model saved at:", model_path)

#Training vs Validation Plot
import matplotlib.pyplot as plt

accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]

loss = history.history["loss"]
val_loss = history.history["val_loss"]

epochs_range = range(1, len(accuracy) + 1)

plt.figure()
plt.plot(epochs_range, accuracy, label="Training Accuracy")
plt.plot(epochs_range, val_accuracy, label="Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

plt.figure()
plt.plot(epochs_range, loss, label="Training Loss")
plt.plot(epochs_range, val_loss, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()


